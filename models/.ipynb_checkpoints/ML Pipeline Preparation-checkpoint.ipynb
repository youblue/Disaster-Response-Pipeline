{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wangshuzhang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangshuzhang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wangshuzhang/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wangshuzhang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table(table_name='message_table', con=engine)\n",
    "X = df['message']\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    # Normalization\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) \n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Stop words removal\n",
    "    words = [w for w in words if words not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Stemming and Lemmatizing\n",
    "    tokens = [PorterStemmer().stem(w) for w in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state = 42)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8030,)\n",
      "(2008,)\n",
      "(8030, 36)\n",
      "(2008, 36)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/wangshuzhang/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/share/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/Users/wangshuzhang/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/share/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9w/0bpzfbws7yng60m1fwfrbpkr0000gn/T/ipykernel_1214/37106743.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9w/0bpzfbws7yng60m1fwfrbpkr0000gn/T/ipykernel_1214/2864425882.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclean_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mclean_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mclean_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             )\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/wangshuzhang/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/share/nltk_data'\n    - '/Users/wangshuzhang/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 1.  0.  1. ...,  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.values[:,0])\n",
    "print(Y_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.84      0.82      1307\n",
      "               request       0.80      0.59      0.68       706\n",
      "                 offer       0.00      0.00      0.00         3\n",
      "           aid_related       0.80      0.59      0.68       785\n",
      "          medical_help       0.45      0.04      0.08       114\n",
      "      medical_products       0.83      0.08      0.15        59\n",
      "     search_and_rescue       0.00      0.00      0.00        36\n",
      "              security       0.00      0.00      0.00        25\n",
      "              military       0.00      0.00      0.00        14\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.86      0.41      0.56       135\n",
      "                  food       0.89      0.58      0.70       280\n",
      "               shelter       0.90      0.32      0.47       197\n",
      "              clothing       0.00      0.00      0.00        15\n",
      "                 money       0.00      0.00      0.00        28\n",
      "        missing_people       0.00      0.00      0.00        20\n",
      "              refugees       0.00      0.00      0.00        30\n",
      "                 death       1.00      0.04      0.08        50\n",
      "             other_aid       0.54      0.05      0.09       297\n",
      "infrastructure_related       0.00      0.00      0.00        58\n",
      "             transport       0.00      0.00      0.00        44\n",
      "             buildings       1.00      0.08      0.14        79\n",
      "           electricity       0.00      0.00      0.00         6\n",
      "                 tools       0.00      0.00      0.00         5\n",
      "             hospitals       0.00      0.00      0.00        10\n",
      "                 shops       0.00      0.00      0.00         8\n",
      "           aid_centers       0.00      0.00      0.00        15\n",
      "  other_infrastructure       0.00      0.00      0.00        30\n",
      "       weather_related       0.89      0.42      0.57       293\n",
      "                floods       1.00      0.20      0.34        64\n",
      "                 storm       1.00      0.08      0.14        53\n",
      "                  fire       0.00      0.00      0.00         5\n",
      "            earthquake       0.85      0.44      0.58       165\n",
      "                  cold       0.00      0.00      0.00         6\n",
      "         other_weather       0.50      0.03      0.06        33\n",
      "         direct_report       0.77      0.54      0.63       682\n",
      "\n",
      "           avg / total       0.75      0.51      0.57      5657\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test.values, Y_pred, target_names=Y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f65a1db5950>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f65a1db5950>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': 42,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.3057285180572852, total=  33.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   55.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.3138231631382316, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.3231631382316314, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.3181818181818182, total=  33.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.3163138231631382, total=  33.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.3125778331257783, total=  39.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.3437110834371108, total=  39.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.3312577833125778, total=  39.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.3306351183063512, total=  39.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.3306351183063512, total=  39.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.3206724782067248, total=  32.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.30884184308841844, total=  31.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.30635118306351183, total=  32.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.2833125778331258, total=  32.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.2957658779576588, total=  32.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.31693648816936487, total=  37.6s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.32254047322540474, total=  38.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.3163138231631382, total=  37.8s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.3119551681195517, total=  37.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.31133250311332505, total=  37.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.2820672478206725, total=  30.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.2671232876712329, total=  30.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.27023661270236615, total=  30.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.27459526774595266, total=  29.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.29389788293897884, total=  29.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.28704856787048566, total=  32.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.28829389788293897, total=  32.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.2764632627646326, total=  33.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.2845579078455791, total=  32.8s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.2982565379825654, total=  32.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.2820672478206725, total=  29.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.2671232876712329, total=  29.5s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.27023661270236615, total=  29.8s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.27459526774595266, total=  29.9s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.29389788293897884, total=  30.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.28704856787048566, total=  33.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.28829389788293897, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.2764632627646326, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.2845579078455791, total=  33.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.2982565379825654, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.263387297633873, total=  29.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.2465753424657534, total=  29.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.24844333748443337, total=  29.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.23412204234122042, total=  29.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.27895392278953923, total=  29.5s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.25965130759651306, total=  32.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.24470734744707348, total=  32.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.2590286425902864, total=  32.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.23848069738480698, total=  31.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.27334993773349936, total=  32.1s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.263387297633873, total=  29.5s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.2465753424657534, total=  29.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.24844333748443337, total=  29.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.23412204234122042, total=  29.6s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.27895392278953923, total=  29.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.25965130759651306, total=  32.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.24470734744707348, total=  31.7s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.2590286425902864, total=  32.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.23848069738480698, total=  32.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_leaf=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.27334993773349936, total=  32.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.17559153175591533, total=  27.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.1581569115815691, total=  27.7s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.19551681195516812, total=  27.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.19240348692403486, total=  27.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.19738480697384808, total=  27.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.17995018679950187, total=  28.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.1581569115815691, total=  28.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.18804483188044832, total=  28.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.19551681195516812, total=  28.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, score=0.19738480697384808, total=  28.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.17559153175591533, total=  27.8s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.1581569115815691, total=  27.7s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.19613947696139478, total=  27.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.19427148194271482, total=  27.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, score=0.19738480697384808, total=  27.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.17995018679950187, total=  28.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.1581569115815691, total=  28.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.18742216687422167, total=  28.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.19863013698630136, total=  28.4s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=1, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=20, score=0.19738480697384808, total=  28.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.17683686176836863, total=  27.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, score=0.15753424657534246, total=  27.2s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_leaf=3, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10 \n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__max_depth':(None,5), \n",
    "    'clf__estimator__min_samples_leaf':(1,3,5),\n",
    "    'clf__estimator__min_samples_split':(2,5), \n",
    "    'clf__estimator__n_estimators':(10,20)\n",
    "} \n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1, verbose=3, cv=5)\n",
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = cv.predict(X_test)\n",
    "print(classification_report(Y_test.values, Y_pred, target_names=Y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            \n",
    "            if len(pos_tags) > 1:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier(random_state = 42)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_d...timator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.82      0.83      1307\n",
      "               request       0.77      0.67      0.72       706\n",
      "                 offer       0.00      0.00      0.00         3\n",
      "           aid_related       0.82      0.69      0.75       785\n",
      "          medical_help       0.48      0.25      0.33       114\n",
      "      medical_products       0.63      0.29      0.40        59\n",
      "     search_and_rescue       0.42      0.14      0.21        36\n",
      "              security       0.00      0.00      0.00        25\n",
      "              military       0.00      0.00      0.00        14\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.81      0.84      0.82       135\n",
      "                  food       0.84      0.76      0.80       280\n",
      "               shelter       0.82      0.69      0.75       197\n",
      "              clothing       0.45      0.33      0.38        15\n",
      "                 money       0.64      0.25      0.36        28\n",
      "        missing_people       0.00      0.00      0.00        20\n",
      "              refugees       0.17      0.03      0.06        30\n",
      "                 death       0.50      0.18      0.26        50\n",
      "             other_aid       0.59      0.27      0.37       297\n",
      "infrastructure_related       0.22      0.03      0.06        58\n",
      "             transport       0.17      0.02      0.04        44\n",
      "             buildings       0.76      0.44      0.56        79\n",
      "           electricity       0.50      0.50      0.50         6\n",
      "                 tools       0.00      0.00      0.00         5\n",
      "             hospitals       0.40      0.20      0.27        10\n",
      "                 shops       0.00      0.00      0.00         8\n",
      "           aid_centers       0.00      0.00      0.00        15\n",
      "  other_infrastructure       0.21      0.10      0.14        30\n",
      "       weather_related       0.84      0.59      0.69       293\n",
      "                floods       0.83      0.30      0.44        64\n",
      "                 storm       0.87      0.38      0.53        53\n",
      "                  fire       0.00      0.00      0.00         5\n",
      "            earthquake       0.90      0.67      0.77       165\n",
      "                  cold       0.25      0.17      0.20         6\n",
      "         other_weather       0.17      0.03      0.05        33\n",
      "         direct_report       0.76      0.66      0.70       682\n",
      "\n",
      "           avg / total       0.75      0.62      0.67      5657\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = pipeline2.predict(X_test)\n",
    "print(classification_report(Y_test.values, Y_pred, target_names=Y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pkl'\n",
    "pickle.dump(pipeline2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
